{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8840035d-f2c4-446d-aaa4-9e6fb6836eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mouad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mouad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mouad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mouad\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66fd1c51-3d81-4b92-9104-f1fa29d3957c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build(self):\\n        train_x, train_y = self.train_data()\\n\\n        # Create a Sequentiol model with 3 layers.\\n\\n        model = Sequential()\\n        model.add(Dense(128, input_shape=(\\n            len(train_x[0]),), activation=\\'relu\\'))\\n\\n        model.add(Dropout(0.5))\\n\\n        model.add(Dense(64, activation=\"relu\"))\\n        model.add(Dropout(0.5))\\n\\n        model.add(Dense(len(train_y[0]), activation=\"softmax\"))\\n\\n        sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\\n        model.compile(loss=\"categorical_crossentropy\",optimizer=sgd, metrics=[\"accuracy\"])\\n\\n        hist = model.fit(np.array(train_x), np.array(train_y),epochs=200, batch_size=10, verbose=1)\\n        \\n        \\n        \\n        \\n        # Save model and words, classes which can be used for prediction\\n        \\n        model.save(\"chatbot_model.h5\",hist)\\n        pickle.dump({\"words\":self.words,\"classes\":self.classes,\\n                    \"train_x\":train_x,\"train_y\":train_y}, open(\"training_data\",\"wb\"))\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "data_file = open(\"intents.json\").read()\n",
    "\n",
    "intents = json.loads(data_file)[\"intents\"]\n",
    "\n",
    "def preprocess(data):\n",
    "    # Tokenize data\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    \n",
    "    # lowercase all the tokens\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    tokens = [w for w in tokens if w not in stop and w not in string.punctuation]\n",
    "    \n",
    "    # Lemmatize words \n",
    "    Lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = [Lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens  \n",
    "'''\n",
    "def build(self):\n",
    "        train_x, train_y = self.train_data()\n",
    "\n",
    "        # Create a Sequentiol model with 3 layers.\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(\n",
    "            len(train_x[0]),), activation='relu'))\n",
    "\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(len(train_y[0]), activation=\"softmax\"))\n",
    "\n",
    "        sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss=\"categorical_crossentropy\",optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "        hist = model.fit(np.array(train_x), np.array(train_y),epochs=200, batch_size=10, verbose=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Save model and words, classes which can be used for prediction\n",
    "        \n",
    "        model.save(\"chatbot_model.h5\",hist)\n",
    "        pickle.dump({\"words\":self.words,\"classes\":self.classes,\n",
    "                    \"train_x\":train_x,\"train_y\":train_y}, open(\"training_data\",\"wb\"))\n",
    "'''     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e535babe-071d-41c9-98ff-e24315c3b903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tag': 'greeting',\n",
       "  'patterns': ['hi', 'is anyone there?', 'Hello', 'Good day'],\n",
       "  'responses': ['Hello , thanks for visiting TechVidvan',\n",
       "   'Good to see you again! ',\n",
       "   'Hi , how can I help?'],\n",
       "  'set': ''},\n",
       " {'tag': 'botstatus',\n",
       "  'patterns': ['How are you doing?', 'Whats up', 'are you fine'],\n",
       "  'responses': ['I am fine.Thanks for asking.', 'I am doing good.'],\n",
       "  'set': ''},\n",
       " {'tag': 'creator',\n",
       "  'patterns': ['who are you ?',\n",
       "   'who is this ?',\n",
       "   'who am i talking to?',\n",
       "   'what is your name?',\n",
       "   'are you human?'],\n",
       "  'responses': [\"Hi my name is TechVidvanBot. 'TechVidvan' team has created me.\\nNice to meet you.\"],\n",
       "  'set': ''},\n",
       " {'tag': 'sad',\n",
       "  'patterns': ['i am sad ', 'i am upset', 'i have a problem', 'i am not okay'],\n",
       "  'responses': ['No problem. Everything will be okay',\n",
       "   \"'There is only one happiness in life, to love and be loved' \\n- George Sand.\",\n",
       "   \"'Cry. Forgive. Learn. Move on. \\nLet your tears water the seeds of your future happiness' \\n- Steve Maraboli\",\n",
       "   'I am here for you, how can I help?'],\n",
       "  'set': ''},\n",
       " {'tag': 'funny',\n",
       "  'patterns': ['you have good sense of humour', 'you are funny bot'],\n",
       "  'responses': ['Yes i have good sense of humour.',\n",
       "   'Thanks for your compliment'],\n",
       "  'set': ''},\n",
       " {'tag': 'positive',\n",
       "  'patterns': ['ok',\n",
       "   'good cool',\n",
       "   'yes wow you are so intelligent ',\n",
       "   'hurray i am happy',\n",
       "   'i am doing good'],\n",
       "  'responses': ['I Hope you are getting your answers cleared.'],\n",
       "  'set': ''},\n",
       " {'tag': 'goodbye',\n",
       "  'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
       "  'responses': ['See you later, thanks for visiting TechVidvan',\n",
       "   'Have a nice day',\n",
       "   'Bye! Come back again soon.']},\n",
       " {'tag': 'thanks',\n",
       "  'patterns': ['Thank you', \"That's helpful\"],\n",
       "  'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n",
       " {'tag': 'order',\n",
       "  'patterns': ['show my order details?', 'Provide my order details', 'orders'],\n",
       "  'responses': ['Do you want Purchased Order or Cart order details?'],\n",
       "  'set': 'orderid'},\n",
       " {'tag': 'Purchased',\n",
       "  'patterns': ['Purchased order', 'Purchased'],\n",
       "  'responses': ['Please provide your order id', 'Please say your order id'],\n",
       "  'filter': 'orderid',\n",
       "  'set': 'historydetails'},\n",
       " {'tag': 'ordernumber',\n",
       "  'patterns': ['ordernumber'],\n",
       "  'responses': ['You ordered Smartwatch'],\n",
       "  'filter': 'historydetails'},\n",
       " {'tag': 'Cart',\n",
       "  'patterns': ['Cart', 'Cart details'],\n",
       "  'responses': ['Your cart contains: \\n1)Watch.\\n2)Clock'],\n",
       "  'filter': 'orderid'},\n",
       " {'tag': 'offers',\n",
       "  'patterns': ['show me the offers', 'What kinds of deals do you have?'],\n",
       "  'responses': ['Are you looking for deals on watches or smartphones?'],\n",
       "  'set': 'productoffers'},\n",
       " {'tag': 'watches',\n",
       "  'patterns': ['watches'],\n",
       "  'responses': ['For about $100, you can get a 10X gold watch.'],\n",
       "  'filter': 'productoffers'},\n",
       " {'tag': 'smartphones',\n",
       "  'patterns': ['smartphones'],\n",
       "  'responses': ['XYZ Phone is available at $75 only.'],\n",
       "  'filter': 'productoffers'},\n",
       " {'tag': 'Products',\n",
       "  'patterns': ['products available', 'show me available items'],\n",
       "  'responses': ['Products available:\\n1) Watch \\n2) Smartphone \\n3) Clock'],\n",
       "  'set': ''}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intents\n",
    "\n",
    "#processed_data = [preprocess(intent) for intent in intents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad803a4-6128-434b-97bb-36ee0044e4fa",
   "metadata": {},
   "source": [
    "## **Train a machine learning model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "454fbdf0-90d0-48a6-a066-e68e747d45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = len(processed_data)\n",
    "\n",
    "\n",
    "## Create a tokenizer\n",
    "tokenizer = Tokenizer.texts_to_sequences(prceess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
